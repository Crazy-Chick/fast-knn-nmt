import numpy as np
import numbers

class KMeans:

    def __init__(self, n_clusters, init='k-means++', n_init=10, max_iter=300, tol=1e-4, random_state=None, algorithm='auto'):
        """
        n_clusters : int
        The number of clusters to form as well as the number of
        centroids to generate.

        init : {'k-means++', 'random'}, callable or array of shape \
            (n_clusters, n_features), default='k-means++'
        Method for initialization:

        'k-means++' : selects initial cluster centers for k-mean
        clustering in a smart way to speed up convergence. See section
        Notes in k_init for more details.

        'random': choose `n_clusters` observations (rows) at random from data
        for the initial centroids.

        n_init : int, default=10
        Number of time the k-means algorithm will be run with different
        centroid seeds. The final results will be the best output of
        n_init consecutive runs in terms of inertia.

        max_iter : int, default=300
        Maximum number of iterations of the k-means algorithm for a
        single run.

        tol : float, default=1e-4
        Relative tolerance with regards to Frobenius norm of the difference
        in the cluster centers of two consecutive iterations to declare
        convergence.

        random_state : int, RandomState instance or None, default=None
        Determines random number generation for centroid initialization. Use
        an int to make the randomness deterministic.

        algorithm : {"auto", "full", "elkan"}, default="auto"
        K-means algorithm to use. The classical EM-style algorithm is "full".
        The "elkan" variation is more efficient on data with well-defined
        clusters, by using the triangle inequality. However it's more memory
        intensive due to the allocation of an extra array of shape
        (n_samples, n_clusters).

        For now "auto" (kept for backward compatibiliy) chooses "elkan"
        """

        self.n_clusters = n_clusters
        self.init = init
        self.n_init = n_init
        self.max_iter = max_iter
        self.tol = tol
        self.copy_x = True
        self.random_state = random_state
        self.algorithm = algorithm

    def _tolerance(self, X, tol):
        variences = np.var(X, axis=0)
        return np.mean(variences) * tol

    def _num_samples(self, x):
        """
        return number of samples in x
        """

        if not hasattr(x, '__len__') and not hasattr(x, 'shape'):
            if hasattr(x, '__array__'):
                x = np.asarray(x)
        
        if hasattr(x, 'shape') and x.shape is not None:
            if isinstance(x.shape[0], numbers.Integral):
                return x.shape[0]

    def check_random_state(self, seed):
        """
        Turn seed into a np.random.RandomState instance

        seed : None, int or instance of RandomState
            If seed is None, return the RandomState singleton used by np.random.
            If seed is an int, return a new RandomState instance seeded with seed.
            If seed is already a RandomState instance, return it.
            Otherwise raise ValueError.
        """
        
        if seed is None or seed is np.random:
            return np.random.mtrand._rand
        if isinstance(seed, numbers.Integral):
            return np.random.RandomState(seed)
        if isinstance(seed, np.random.RandomState):
            return seed
    
    def row_norms(self, X, squared=False):
        """
        Row-wise (squared) Euclidean norm of X.

        Equivalent to np.sqrt((X * X).sum(axis=1))

        Parameters
        ----------
        X : The input array.

        squared : bool, default=False
            If True, return squared norms.

        Returns
        -------
        The row-wise (squared) Euclidean norm of X.
        """

        norms = np.einsum('ij,ij->i', X, X)
        
        if not squared:
            np.sort(norms, norms)
        return norms
    
    def _euclidean_distances(self, X, Y, x_squared_norms=None, y_squared_norms=None, squared=False):
        """
        Computational part of euclidean_distances
        """

        if x_squared_norms is not None:
            XX = x_squared_norms.reshape(-1, 1)
        else:
            XX = self.row_norms(X, squared=True)[:, np.newaxis]
        
        if Y is X:
            YY = None if XX is None else XX.T
        else:
            if y_squared_norms is not None:
                YY = y_squared_norms.reshape(1, -1)
            else:
                YY = self.row_norms(Y, squared=True)[np.newaxis, :]
        
        dot_max = X @ Y.T
        distances = - 2 * dot_max
        distances += XX
        distances += YY
        np.maximum(distances, 0, out=distances)

        if X is Y:
            np.fill_diagonal(distances, 0)
        
        return distances if squared else np.sqrt(distances, out=distances)

    def _vec_euclidean(self, x, y, squared):
        distance = (x-y) * (x-y).T
        return distance if squared else np.sqrt(distance)
    
    def _init_bounds(self, X, centers, center_half_distances, labels, upper_bounds, lower_bounds):
        """
        Initialize upper and lower bounds for each sample for input data.

        Given X, centers and the pairwise distances divided by 2.0 between the
        centers this calculates the upper bounds and lower bounds for each sample.
        The upper bound for each sample is set to the distance between the sample
        and the closest center.

        The lower bound for each sample is a one-dimensional array of n_clusters.
        For each sample i assume that the previously assigned cluster is c1 and the
        previous closest distance is dist, for a new cluster c2, the
        lower_bound[i][c2] is set to distance between the sample and this new
        cluster, if and only if dist > center_half_distances[c1][c2]. This prevents
        computation of unnecessary distances for each sample to the clusters that
        it is unlikely to be assigned to.

        Parameters
        ----------
        X : array of shape (n_samples, n_features), dtype=float64
            The input data.

        centers : array of shape (n_clusters, n_features), dtype=float64
            The cluster centers.

        center_half_distances : array of shape (n_clusters, n_clusters) dtype=float64
            The half of the distance between any 2 clusters centers.

        labels : array of shape(n_samples), dtype=int
            The label for each sample. This array is modified in place.

        upper_bounds : array of shape(n_samples,), dtype=float64
            The upper bound on the distance between each sample and its closest
            cluster center. This array is modified in place.

        lower_bounds : array, of shape(n_samples, n_clusters), dtype=float64
            The lower bound on the distance between each sample and each cluster
            center. This array is modified in place.
        """

        n_samples = X.shape[0]
        n_clusters = centers.shape[0]

        for i in range(n_samples):
            best_cluster = 0
            min_dist = self._vec_euclidean(X[i], centers[0], squared=False)

            lower_bounds[i, 0] = min_dist
            for j in range(1, n_clusters):
                if min_dist > center_half_distances[best_cluster, j]:
                    dist = self._vec_euclidean(X[i], centers[j], squared=False)
                    lower_bounds[i, j] = dist
                    if dist < min_dist:
                        min_dist = dist
                        best_cluster = j
            labels[i] = best_cluster
            upper_bounds[i] = min_dist
    
    def _elkan_each_itr(self, X, sample_weight, centers_old, center_half_distances, distance_next_center, labels, upper_bounds, lower_bounds, centers_new, weight_in_clusters, update_centers=True):
        """
        K-means combined EM step
         """
        
        n_samples = X.shape[0]
        n_features = X.shape[1]
        n_clusters = centers_new.shape[0]

        for i in range(n_samples):
            upper_bound = upper_bounds[i]
            bounds_tight = 0
            label = labels[i]

            if not distance_next_center[label] >= upper_bound:
                for j in range(n_clusters):
                    if (j != label and (upper_bound > lower_bounds[i, j]) and (upper_bound > center_half_distances[label, j])):
                        if not bounds_tight:
                            upper_bound = self._vec_euclidean(X[i], centers_old[label], squared=False)
                            lower_bounds[i, label] = upper_bound
                            bounds_tight = 1
                        
                        if (upper_bound > lower_bounds[i, j] or (upper_bound > center_half_distances[label, j])):
                            distance = self._vec_euclidean(X[i], centers_old[j], squared=False)
                            lower_bounds[i, j] = distance
                            if distance < upper_bound:
                                label = j
                                upper_bound = distance
                labels[i] = label
                upper_bounds[i] = upper_bound

    def _kmeans_plusplus(self, X, n_clusters, x_squared_norms, random_state):
        """
        Computational component for initialization of n_clusters by
        k-means++. 

        Parameters
        ----------
        X : array of shape (n_samples, n_features)
            The data to pick seeds for.

        n_clusters : int
            The number of seeds to choose.

        x_squared_norms : array of shape (n_samples,)
            Squared Euclidean norm of each data point.

        random_state : RandomState instance
            The generator used to initialize the centers.

        Returns
        -------
        centers : array of shape (n_clusters, n_features)
            The inital centers for k-means.

        indices : array of shape (n_clusters,)
            The index location of the chosen centers in the data array X. For a
            given index and center, X[index] = center.
        """

        n_samples, n_features = X.shape

        centers = np.empty((n_clusters, n_features), dtype=X.dtype)

        n_local_trials = 2 + int(np.log(n_clusters))

        center_id = random_state.randint(n_samples)
        indices = np.full(n_clusters, -1, dtype=int)
        centers[0] = X[center_id]
        indices[0] = center_id
        
        # Initialize list of closest distances and calculate current potential
        closest_dist_sq = self._euclidean_distances(centers[0, np.newaxis], X, y_squared_norms=x_squared_norms, squared=True)
        current_pot = closest_dist_sq.sum()

        # pick the remaining n_clusters-1 points
        for c in range(1, n_clusters):
            rand_vals = random_state.random_sample(n_local_trials) * current_pot
            candidate_ids = np.searchsorted(np.cumsum(closest_dist_sq, axis=None, dtype=np.float64), rand_vals)
            np.clip(candidate_ids, None, closest_dist_sq.size-1, out=candidate_ids)

            # compute distances to center candidates
            distance_to_candidates = self._euclidean_distances(X[candidate_ids], X, y_squared_norms=x_squared_norms, squared=True)

            # upadte closet distances squared and potential for each candidate
            np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)
            candidates_pot = distance_to_candidates.sum(axis=1)

            # decide which candidate is the best
            best_candidate = np.argmin(candidates_pot)
            current_pot = candidates_pot[best_candidate]
            closest_dist_sq = distance_to_candidates[best_candidate]
            best_candidate = candidate_ids[best_candidate]

            centers[c] = X[best_candidate]
            indices[c] = best_candidate

        return centers, indices
        
    
    def _init_centroids(self, X, x_squared_norms, init, random_state, init_size=None):
        """
        Compute the initial centroids.

        Parameters
        ----------
        X : array of shape (n_samples, n_features)
            The input samples.

        x_squared_norms : array of shape (n_samples,)
            Squared euclidean norm of each data point.

        init : {'k-means++', 'random'}, callable or ndarray of shape \
                (n_clusters, n_features)
            Method for initialization.

        random_state : RandomState instance
            Determines random number generation for centroid initialization.
            See :term:`Glossary <random_state>`.

        init_size : int, default=None
            Number of samples to randomly sample for speeding up the
            initialization (sometimes at the expense of accuracy).

        Returns
        -------
        centers : array of shape (n_clusters, n_features)
        """

        n_samples = X.shape[0]
        n_clusters = self.n_clusters

        if init_size is not None and init_size < n_samples:
            init_indices = random_state.randint(0, n_samples, init_size)
            X = X[init_indices]
            x_squared_norms = x_squared_norms[init_indices]
            n_samples = X.shape[0]
        
        if isinstance(init, str) and init == "k-means++":
            centers, _ = self._kmeans_plusplus(X, n_clusters, random_state=random_state, x_squared_norms=x_squared_norms)
        else:
            seeds = random_state.permutation(n_samples)[:n_clusters]
            centers = X[seeds]
        
        return centers
    
    def _kmeans_signle_elkan(self, X, sample_weight, centers_init, max_iter=300, x_squared_norms=None, tol=1e-4):
        """
        A single run of k-means elkan.

        Parameters
        ----------
        X : array of shape (n_samples, n_features)
            The observations to cluster.

        sample_weight : array of shape (n_samples,)
            The weights for each observation in X.

        centers_init : ndarray of shape (n_clusters, n_features)
            The initial centers.

        max_iter : int, default=300
            Maximum number of iterations of the k-means algorithm to run.

        x_squared_norms : array, default=None
            Precomputed x_squared_norms.

        tol : float, default=1e-4
            Relative tolerance with regards to Frobenius norm of the difference
            in the cluster centers of two consecutive iterations to declare
            convergence.

        Returns
        -------
        centroid : array of shape (n_clusters, n_features)
            Centroids found at the last iteration of k-means.

        label : array of shape (n_samples,)
            label[i] is the code or index of the centroid the
            i'th observation is closest to.

        n_iter : int
            Number of iterations run.
        """

        n_samples = X.shape[0]
        n_clusters = centers_init.shape[0]

        centers = centers_init
        centers_new = np.zeros_like(centers)
        weight_in_clusters = np.zeros(n_clusters, dtype=X.dtype)
        labels = np.full(n_samples, -1, dtype=np.int32)
        labels_old = labels.copy()
        center_half_distances = self._euclidean_distances(centers, centers.copy()) / 2
        distance_next_center = np.partition(np.asarray(center_half_distances), kth=1, axis=0)[1]
        upper_bounds = np.zeros(n_samples, dtype=X.dtype)
        lower_bounds = np.zeros((n_samples, n_clusters), dtype=X.dtype)
        center_shift = np.zeros(n_clusters, dtype=X.dtype)

        init_bounds = self._init_bounds
        elkan_iter = self.elkan_iter_chunked_dense
        _inertia = self._inertia_dense

        init_bounds(X, centers, center_half_distances, labels, upper_bounds, lower_bounds)

        strict_convergence = False
        
        for i in range(max_iter):
            elkan_iter(X, sample_weight, centers, centers_new, weight_in_clusters, center_half_distances, distance_next_center, center_half_distances, distance_next_center, upper_bounds, lower_bounds, labels, center_shift, update_centers=True)
            
            center_half_distances = self._euclidean_distances(centers_new, centers_new.copy()) / 2
            distance_next_center = np.partition(np.asarray(center_half_distances), kth=1, axis=0)[1]

            centers, centers_new = centers_new, centers

            if np.array_equal(labels, labels_old):
                strict_convergence = True
                break
            else:
                center_shift_tot = (center_shift**2).sum()
                if center_shift_tot <= tol:
                    break

            labels_old[:] = labels

        if not strict_convergence:
            elkan_iter(X, sample_weight, centers, centers, weight_in_clusters, center_half_distances, distance_next_center, upper_bounds, lower_bounds, labels, center_shift, update_centers=False)
        
        inertia = _inertia(X, sample_weight, centers, labels)

        return labels, inertia, centers, i + 1
                


    def fit(self, X):
        """
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Training instances to cluster.
        """

        X = X.astype("float64")
        random_state = self.check_random_state(self.random_state)
        sample_weight = np.ones(self._num_samples(X), dtype=X.dtype)
        init = self.init
        
        # subtract of mean of x for more accurate distance computations
        X_mean = X.mean(axis=0)
        X -= X_mean

        x_squared_norms = self.row_norms(X, squared=True)

        if (self.algorithm != "full"):
            kmeans_single = self._kmeans_signle_elkan
        else:
            kmeans_single = self._kmeans_signle_lloyd
        
        best_inertia = None
        
        for i in range(self.n_init):
            # initialize centers
            centers_init = self._init_centroids(X, x_squared_norms=x_squared_norms, init=init, random_state=random_state)

            # run k-means
            labels, inertia, centers, n_iter_ = kmeans_single(X, sample_weight, centers_init, max_iter=self.max_iter, tol=self.tol, x_squared_norms=x_squared_norms)

            if (best_inertia is None or inertia < best_inertia):
                best_labels = labels
                best_centers = centers
                best_inertia = inertia
                best_n_iter = n_iter_
            
        if not self.copy_x:
            X += X_mean
        best_centers += X_mean

        self.clusetr_centers_ = best_centers
        self.labels_ = best_labels
        self.inertia_ = best_inertia
        self.n_iter_ = best_n_iter
