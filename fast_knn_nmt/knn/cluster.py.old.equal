import os
import argparse
import json
import numpy as np
from tqdm import tqdm
import multiprocessing

from fast_knn_nmt.data.utils import warmup_mmap_file
from fast_knn_nmt.knn.k_means import KMeans

def get_info(data_store):
    new_line = ""
    with open(os.path.join(data_store, "info.json"), "r") as f:
        for line in f:
            new_line += line
        f.close()
    return json.loads(new_line)

def build_cluster(data_store, cluster_size, tol, queue=None):
    info = get_info(data_store)
    key_file = os.path.join(data_store, "keys.npy")
    val_file = os.path.join(data_store, "vals.npy")

    warmup_mmap_file(key_file, verbose=False)
    warmup_mmap_file(val_file, verbose=False)

    keys = np.memmap(key_file, 
                     dtype=np.float16 if info['dstore_fp16'] else np.float32,
                     mode="r",
                     shape=(info['dstore_size'], info['hidden_size']))
    vals = np.memmap(val_file,
                     dtype=np.int16 if info['dstore_fp16'] else np.int32,
                     mode="r",
                     shape=(info['dstore_size'], info['val_size']))

    n_cluster = info['dstore_size'] // cluster_size
    if (n_cluster * cluster_size != info['dstore_size']):
        n_cluster += 1
    k_means = KMeans(n_cluster, tol=tol)
    centers, clustser_label, clustser_dis = k_means.fit(keys, cluster_size)

    cluster_file = os.path.join(data_store, "cluster_center.npy")
    cluster_mmap = np.memmap(cluster_file, 
                     dtype=np.float16 if info['dstore_fp16'] else np.float32,
                     mode="w+",
                     shape=(n_cluster, info['hidden_size']))
    cluster_mmap[:] = centers[:]
    
    center_has = [[] for i in range(n_cluster)]
    for i in range(info['dstore_size']):
        center_has[int(clustser_label[i])].append(i)
    
    for idx_, point_list in enumerate(center_has):
        cluster_key_file = os.path.join(data_store, f"cluster_key_{idx_}.npy")
        cluster_key = np.memmap(cluster_key_file, 
                                 dtype=np.float16 if info['dstore_fp16'] else np.float32,
                                 mode="w+",
                                 shape=(len(point_list), info['hidden_size']))

        cluster_val_file = os.path.join(data_store, f"cluster_val_{idx_}.npy")
        cluster_val = np.memmap(cluster_val_file,
                                 dtype=np.int16 if info['dstore_fp16'] else np.int32,
                                 mode="w+",
                                 shape=(len(point_list), info['val_size']))
        
        for i, point in enumerate(point_list):
            cluster_key[i, :] = keys[point, :]
            cluster_val[i, :] = vals[point, :]

    cluster_info = {
        "n_cluster": n_cluster,
        "hidden_size": info['hidden_size'],
        "val_size": info['val_size'],
        "cluster_size": [len(point_list) for point_list in center_has],
        "dstore_fp16": info['dstore_fp16'] 
    }
    json.dump(cluster_info, open(os.path.join(data_store, "cluster_info.json"), "w"),
                  sort_keys=True, indent=4, ensure_ascii=False)
    if queue is not None:
        queue.put(1)

def main():
    parser = argparse.ArgumentParser(description='building cluster')
    parser.add_argument("--dstore-dir", type=str, help="paths to data store. if provided multiple,"
                                                                      "use ',' as separator")
    parser.add_argument('--cluster-size', type=int, default=512, help='cluster size')
    parser.add_argument('--cutoff', type=float, default=0.2, help='control converged')
    parser.add_argument('--num-workers', type=int, default=0, help="thread_num")
    
    args = parser.parse_args()

    '''
    args.dstore_dir = "/data/wangshuhe/fast_knn/multi_domain_paper/koran/bpe/de-en-bin/train_de_data_stores"
    args.cluster_size = 512
    '''
    file_list = []
    files = os.listdir(args.dstore_dir)  
    for f in files:    
        file = os.path.join(args.dstore_dir, f)    
        if os.path.isdir(file):
            file_list.append(file)
    
    #file_list = ['/data/wangshuhe/fast_knn/multi_domain_paper/koran/bpe/de-en-bin/train_de_data_stores/token_4']
    
    if (args.num_workers <= 1):
        with tqdm(total=len(file_list)) as pbar:
            for file in file_list:
                '''
                print("====")
                print(file)
                print("====")
                '''
                build_cluster(data_store=file, cluster_size=args.cluster_size, tol=args.cutoff)
                pbar.update(1)
    else:
        pool = multiprocessing.Pool(args.num_workers)
        queue = multiprocessing.Manager().Queue()

        for file in file_list:
            pool.apply_async(build_cluster, args=(file, args.cluster_size, args.cutoff, queue,))
        
        pool.close()

        pbar = tqdm(total=len(file_list))
        cnt = 0
        while (True):
            cnt += queue.get()
            pbar.update(1)
            if (cnt >= len(file_list)):
                break
        pbar.close()
        pool.join()



if __name__ == '__main__':
    main()